#-------------------------------------------------------------------------------
# Name:        åŸºç¤æª¢ç´¢å¢å¼·ç”Ÿæˆ (Navie RAG) ç¯„ä¾‹ç¨‹å¼
# Purpose:     å±•ç¤º RAG çš„åŸºæœ¬æµç¨‹ã€æä¾›å¯¦ä½œåƒè€ƒï¼Œç‚ºå¾ŒçºŒæ›´é€²éšçš„ RAG ç ”ç©¶å’Œé–‹ç™¼å¥ å®šäº†åŸºç¤ã€‚
#              
# Author:      jkloip
#
# Created:     07/02/2025
# Copyright:   (c) jkloip 2025
#-------------------------------------------------------------------------------

# ç¬¬1æ­¥-å¼•ç”¨æ¨¡çµ„ï¼Œé€£æ¥ OpenAIã€Groqã€Googleã€Ollama çš„ LLM æ¨¡å‹
import os  # å¼•å…¥ Python ç³»çµ±æ¨¡çµ„ï¼Œç”¨ä¾†é€²è¡Œèˆ‡ä½œæ¥­ç³»çµ±ç›¸é—œçš„æ“ä½œï¼Œå¦‚è®€å–ç’°å¢ƒè®Šæ•¸ã€è·¯å¾‘è™•ç†ç­‰
# pip install python-dotenv  å®‰è£ python-dotenv å¥—ä»¶ï¼Œç”¨ä¾†è®€å– .env æª”æ¡ˆä¸­çš„ç’°å¢ƒè®Šæ•¸
from dotenv import load_dotenv  # å¾ dotenv å¥—ä»¶å¼•å…¥ load_dotenv å‡½å¼ï¼Œæ­¤å‡½å¼å¯ä»¥å¾ .env æª”æ¡ˆä¸­è®€å–ç’°å¢ƒè®Šæ•¸ï¼Œä¸éœ€å°‡æ•æ„Ÿè³‡è¨Šç¡¬ç·¨ç¢¼åœ¨ç¨‹å¼ä¸­
# pip install rich  å®‰è£ rich å¥—ä»¶ï¼Œç”¨ä¾†æ ¼å¼åŒ–è¼¸å‡ºã€é¡¯ç¤ºå½©è‰²æ–‡å­—ç­‰
from rich import print as richprint  # å¾ rich å¥—ä»¶åŒ¯å…¥ print å‡½å¼ï¼Œä¸¦é‡æ–°å‘½åç‚º richprintï¼Œå¯ç”¨ä¾†æ ¼å¼åŒ–è¼¸å‡ºã€é¡¯ç¤ºå½©è‰²æ–‡å­—ç­‰

# pip install langchain_openai  langchain_groq  langchain_google_genai  langchain_ollama å®‰è£ langchain_openaiã€langchain_groqã€langchain_google_genaiã€langchain_ollama å¥—ä»¶
from langchain_openai import ChatOpenAI  # å¾ langchain_openai æ¨¡çµ„å¼•å…¥ ChatOpenAI é¡åˆ¥ï¼Œæ­¤é¡åˆ¥å¯ç”¨æ–¼èˆ‡ OpenAI çš„èŠå¤©æ¨¡å‹åšäº’å‹•
from langchain_groq import ChatGroq  # å¾ langchain_groq æ¨¡çµ„å¼•å…¥ ChatGroq é¡åˆ¥ï¼Œæ­¤é¡åˆ¥ç”¨æ–¼æ“ä½œ Groq å¹³å°ä¸Šçš„èŠå¤©æ¨¡å‹åšäº’å‹•
from langchain_google_genai import ChatGoogleGenerativeAI  # å¾ langchain_google_genai æ¨¡çµ„å¼•å…¥ ChatGoogleGenerativeAI é¡åˆ¥ï¼Œå¯ç”¨æ–¼è·Ÿ Google çš„èŠå¤©æ¨¡å‹åšäº’å‹•
from langchain_ollama import ChatOllama  # å¾ langchain_ollama æ¨¡çµ„å¼•å…¥ ChatOllama é¡åˆ¥ï¼Œæ­¤é¡åˆ¥å¯ç”¨æ–¼èˆ‡ Ollama çš„èŠå¤©æ¨¡å‹åšäº’å‹•

load_dotenv()  # å‘¼å« load_dotenv() å‡½å¼ä»¥å¾ .env æª”æ¡ˆä¸­è¼‰å…¥ç’°å¢ƒè®Šæ•¸ï¼Œè®“ç¨‹å¼å¯ä»¥ä½¿ç”¨è¨­å®šå¥½çš„ API é‡‘é‘°æˆ–å…¶ä»–è¨­å®šå€¼

# OpenAI GPT-4o-mini
def openai_generate_response(prompt):
    # å®šç¾© openai_generate_response å‡½å¼ï¼Œä½¿ç”¨ OpenAI çš„ GPT-4o-mini æ¨¡å‹ç”Ÿæˆå›æ‡‰ã€‚
    # è¼¸å…¥åƒæ•¸ prompt ç‚ºä½¿ç”¨è€…æˆ–æ‡‰ç”¨ç¨‹å¼æä¾›çš„æç¤ºæ–‡å­—ã€‚
    
    chat_openai = ChatOpenAI(
        model="gpt-4o-mini", 
        api_key=os.getenv("OPENAI_API_KEY"), 
        temperature=0
    )  
    # å»ºç«‹ä¸€å€‹ ChatOpenAI ç‰©ä»¶ï¼Œè¨­å®š:
    # 1. æ¨¡å‹åç¨±ç‚º "gpt-4o-mini"
    # 2. ä½¿ç”¨ os.getenv("OPENAI_API_KEY") å¾ç’°å¢ƒè®Šæ•¸ä¸­å–å¾— API é‡‘é‘°
    # 3. temperature=0 è¡¨ç¤ºç”Ÿæˆå›æ‡‰æ™‚æ¡ç”¨ä½æº«åº¦ï¼Œä½¿ç”Ÿæˆçµæœè¼ƒç©©å®š
    
    response = chat_openai.invoke(prompt)  
    # å‘¼å« chat_openai ç‰©ä»¶çš„ invoke æ–¹æ³•ï¼Œå‚³å…¥æç¤ºæ–‡å­— promptï¼Œé–‹å§‹ç”Ÿæˆå›æ‡‰
    # æ­¤æ–¹æ³•é‹ä½œæ–¹å¼ç‚ºéä¸²æµæ¨¡å¼ï¼Œå°‡å®Œæ•´å›æ‡‰ä¸€æ¬¡æ€§è¿”å›
    
    return response  
    # å°‡æ¨¡å‹ç”Ÿæˆçš„å›æ‡‰è¿”å›çµ¦å‘¼å«æ–¹

# Groq Llama3-8b-8192
def groq_generate_response(prompt):
    # å®šç¾© groq_generate_response å‡½å¼ï¼Œä½¿ç”¨ Groq å¹³å°çš„ Llama3-8b-8192 æ¨¡å‹ç”Ÿæˆå›æ‡‰
    # è¼¸å…¥åƒæ•¸ prompt ç‚ºæç¤ºæ–‡å­—
    
    chat_groq = ChatGroq(
        model="llama3-8b-8192", 
        api_key=os.getenv("GROQ_API_KEY"), 
        temperature=0
    )  
    # å»ºç«‹ä¸€å€‹ ChatGroq ç‰©ä»¶ï¼Œè¨­å®š:
    # 1. æ¨¡å‹åç¨±ç‚º "llama3-8b-8192"
    # 2. å¾ç’°å¢ƒè®Šæ•¸ä¸­è®€å– GROQ_API_KEY ä½œç‚º API é‡‘é‘°
    # 3. temperature=0 åŒæ¨£ä¿è­‰ç”Ÿæˆå›æ‡‰çš„ç©©å®šæ€§
    
    response = chat_groq.invoke(prompt)  
    # å‘¼å« chat_groq ç‰©ä»¶çš„ invoke æ–¹æ³•ï¼Œå‚³å…¥æç¤ºæ–‡å­— promptï¼Œç”Ÿæˆç›¸æ‡‰çš„å›æ‡‰    
    # æ­¤æ–¹æ³•é‹ä½œæ–¹å¼ç‚ºéä¸²æµæ¨¡å¼ï¼Œå°‡å®Œæ•´å›æ‡‰ä¸€æ¬¡æ€§è¿”å›
    
    return response  
    # å°‡æ¨¡å‹ç”Ÿæˆçš„å›æ‡‰è¿”å›çµ¦å‘¼å«æ–¹

# Google gemini-1.5-flash
def google_generate_response(prompt):
    # å®šç¾© google_generate_response å‡½å¼ï¼Œä½¿ç”¨ Google AI Studio å¹³å°çš„ gemini-1.5-flash-8b æ¨¡å‹ç”Ÿæˆå›æ‡‰
    # è¼¸å…¥åƒæ•¸ prompt ç‚ºæç¤ºæ–‡å­—

    chat_google = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash", 
        api_key=os.getenv("GEMINI_API_KEY"), 
        temperature=0
    )
    # å»ºç«‹ä¸€å€‹ ChatGoogleGenerativeAI ç‰©ä»¶ï¼Œè¨­å®š:
    # 1. æ¨¡å‹åç¨±ç‚º "gemini-1.5-flash-8b"
    # 2. å¾ç’°å¢ƒè®Šæ•¸ä¸­è®€å– GEMINI_API_KEY ä½œç‚º API é‡‘é‘°
    # 3. temperature=0 åŒæ¨£ä¿è­‰ç”Ÿæˆå›æ‡‰çš„ç©©å®šæ€§

    response=chat_google.invoke(prompt) # å•Ÿç”¨éä¸²æµæ¨¡å¼
    # å‘¼å« chat_google ç‰©ä»¶çš„ invoke æ–¹æ³•ï¼Œå‚³å…¥æç¤ºæ–‡å­— promptï¼Œç”Ÿæˆç›¸æ‡‰çš„å›æ‡‰    
    # æ­¤æ–¹æ³•é‹ä½œæ–¹å¼ç‚ºéä¸²æµæ¨¡å¼ï¼Œå°‡å®Œæ•´å›æ‡‰ä¸€æ¬¡æ€§è¿”å›

    return response
    # å°‡æ¨¡å‹ç”Ÿæˆçš„å›æ‡‰è¿”å›çµ¦å‘¼å«æ–¹

# Ollama Llama3.2 1B
def ollama_generate_response(prompt):
    # å®šç¾© ollama_generate_response å‡½å¼ï¼Œä½¿ç”¨ local ollama å¹³å°çš„ llama3.2:1b æ¨¡å‹ç”Ÿæˆå›æ‡‰
    # è¼¸å…¥åƒæ•¸ prompt ç‚ºæç¤ºæ–‡å­—

    chat_ollama = ChatOllama(
        base_url='http://localhost:11434',
        model="llama3.2:1b", 
        temperature=0
    )
    # å»ºç«‹ä¸€å€‹ ChatOllama ç‰©ä»¶ï¼Œè¨­å®š:
    # 1. æ¨¡å‹åç¨±ç‚º "llama3.2:1b"
    # 2. base_url='http://localhost:11434' æŒ‡å®š Ollama æœå‹™çš„ URL
    # 3. temperature=0 åŒæ¨£ä¿è­‰ç”Ÿæˆå›æ‡‰çš„ç©©å®šæ€§

    response=chat_ollama.invoke(prompt) # å•Ÿç”¨éä¸²æµæ¨¡å¼
    # å‘¼å« chat_ollama ç‰©ä»¶çš„ invoke æ–¹æ³•ï¼Œå‚³å…¥æç¤ºæ–‡å­— promptï¼Œç”Ÿæˆç›¸æ‡‰çš„å›æ‡‰    
    # æ­¤æ–¹æ³•é‹ä½œæ–¹å¼ç‚ºéä¸²æµæ¨¡å¼ï¼Œå°‡å®Œæ•´å›æ‡‰ä¸€æ¬¡æ€§è¿”å›

    return response 
    # å°‡æ¨¡å‹ç”Ÿæˆçš„å›æ‡‰è¿”å›çµ¦å‘¼å«æ–¹

# æ¸¬è©¦ LLM æ¨¡å‹å›æ‡‰
# richprint(groq_generate_response("What is the capital of France?"))  # ä½¿ç”¨ LLM æ¨¡å‹å›ç­”ã€Œæ³•åœ‹çš„é¦–éƒ½æ˜¯å“ªè£¡ï¼Ÿã€

# ç¬¬2æ­¥-å¼•ç”¨æ–‡å­—åˆ‡å‰²å™¨æ¨¡çµ„ï¼Œå°‡é•·æ–‡åˆ‡å‰²æˆå¤šå€‹çŸ­æ–‡å€å¡Š
# pip install langchain-text-splitters  å®‰è£ langchain-text-splitters å¥—ä»¶ï¼Œç”¨ä¾†é€²è¡Œæ–‡å­—åˆ‡å‰²
from langchain_text_splitters import CharacterTextSplitter  # åŒ¯å…¥ CharacterTextSplitter é¡åˆ¥

# è®€å–ã€Œ2024ç¾åœ‹åœ‹æƒ…å’¨æ–‡ã€æª”æ¡ˆå…§å®¹ï¼Œè©²æª”æ¡ˆå…§å®¹ç‚ºæ•´ç¯‡ç¾åœ‹åœ‹æƒ…å’¨æ–‡çš„æ–‡å­—
with open("2024_state_of_the_union.txt", encoding='utf-8') as f:  # ä½¿ç”¨ UTF-8 ç·¨ç¢¼é–‹å•Ÿæª”æ¡ˆ
    state_of_the_union = f.read()  # å°‡æª”æ¡ˆä¸­çš„å…§å®¹è®€å–ç‚ºä¸€å€‹å¤§å­—ä¸²è®Šæ•¸

# æ¸¬è©¦åˆ—å°åœ‹æƒ…å’¨æ–‡çš„å‰ 1000 å€‹å­—å…ƒï¼Œä»¥ç¢ºèªè®€å–æ˜¯å¦æˆåŠŸ
# richprint(state_of_the_union[:1000])  # æ­¤è¡ŒåŸå…ˆå°å‡ºåœ‹æƒ…å’¨æ–‡çš„å‰ 1000 å€‹å­—å…ƒ
# print(type(state_of_the_union))  # åˆ—å° state_of_the_union è®Šæ•¸çš„è³‡æ–™å‹åˆ¥

# åˆå§‹åŒ–æ–‡å­—åˆ‡å‰²å™¨ï¼Œè¨­å®šåˆ‡å‰²åƒæ•¸
text_splitter = CharacterTextSplitter(
    chunk_size=512,    # æ¯å€‹åˆ‡å‰²å¾Œçš„å€å¡Šæœ€å¤§åŒ…å« 512 å€‹å­—å…ƒ
    chunk_overlap=100,  # å€å¡Šä¹‹é–“æœ‰ 100 å€‹å­—å…ƒçš„é‡ç–Šï¼Œè®“ä¸Šä¸‹æ–‡å¾—ä»¥é€£è²«
    length_function=len # ä½¿ç”¨å…§å»ºçš„ len å‡½å¼è¨ˆç®—å­—ä¸²é•·åº¦
)

# æ¸¬è©¦åˆ—å°æ–‡å­—åˆ‡å‰²å™¨çš„è³‡æ–™å‹åˆ¥
# print(type(text_splitter))  # å°å‡º text_splitter è®Šæ•¸çš„è³‡æ–™å‹åˆ¥

# ä½¿ç”¨ text_splitter æ–‡å­—åˆ‡å‰²å™¨å°‡æ•´ç¯‡åœ‹æƒ…å’¨æ–‡åˆ‡å‰²æˆå¤šå€‹å€å¡Š
# è£œå……ï¼š create_documents æ–¹æ³•å‚³å…¥çš„åƒæ•¸ç‚ºä¸€å€‹åˆ—è¡¨ï¼Œè£¡é¢å¯ä»¥åŒ…å«å¤šå€‹æ–‡ä»¶ã€‚ç”¨ `[...]` å°‡å­—ä¸²åŒ…èµ·ä¾†ï¼Œå¯å°‡å–®ä¸€å­—ä¸²è½‰æ›æˆå«æœ‰ä¸€å€‹å…ƒç´ çš„åˆ—è¡¨
texts = text_splitter.create_documents([state_of_the_union])  # å‚³å…¥åŒ…å«å…¨æ–‡çš„ä¸²åˆ—ä¾†ç”¢ç”Ÿå¤šå€‹æ–‡æª”å€å¡Š

# æ¸¬è©¦åˆ—å°åˆ‡å‰²å¾Œçš„æ–‡æª”å€å¡Šï¼Œä»¥ç¢ºèªåˆ‡å‰²æ˜¯å¦æˆåŠŸ
# richprint(texts)  # æ­¤è¡ŒåŸå…ˆå°å‡ºæ‰€æœ‰åˆ‡å‰²å¾Œçš„å€å¡Š
# print(len(texts))  # åˆ—å°åˆ‡å‰²å¾Œçš„æ–‡æª”å€å¡Šæ•¸é‡
# print(type(texts))  # åˆ—å°åˆ‡å‰²å¾Œçš„æ–‡æª”å€å¡Šè³‡æ–™å‹åˆ¥

# ç¬¬3æ­¥-å¼•ç”¨å‘é‡è³‡æ–™åº«æ¨¡çµ„ Chroma èˆ‡ OpenAIã€Hugging Face çš„å‘é‡åµŒå…¥æ¨¡çµ„ Embeddingsï¼Œå°‡æ–‡æª”å€å¡Šè½‰æ›ç‚ºå‘é‡è¡¨ç¤º
# åŒ¯å…¥å‘é‡è³‡æ–™åº«æ¨¡çµ„ Chroma èˆ‡ OpenAI çš„å‘é‡åµŒå…¥ï¼ˆEmbeddingsï¼‰æ¨¡çµ„
# pip install langchain-chroma  å®‰è£ langchain-chroma å¥—ä»¶ï¼Œç”¨ä¾†å»ºç«‹å‘é‡è³‡æ–™åº«
from langchain_chroma import Chroma  # åŒ¯å…¥ Chromaï¼Œä½œç‚ºå‘é‡è³‡æ–™åº«ç®¡ç†å·¥å…·
from langchain_openai import OpenAIEmbeddings  # åŒ¯å…¥ OpenAIEmbeddingsï¼Œç”¨ä¾†å°‡æ–‡å­—è½‰æ›æˆå‘é‡è¡¨ç¤º

# å»ºç«‹ OpenAI åµŒå…¥æ¨¡å‹ï¼ŒæŒ‡å®šä½¿ç”¨ "text-embedding-3-large" æ¨¡å‹ä¾†è½‰æ›æ–‡å­—ç‚ºæ•¸å€¼å‘é‡
# openai_embeddings = OpenAIEmbeddings(model="text-embedding-3-large") # æ•ˆæœä½³ï¼Œä½†è¦èŠ±éŒ¢

# ä½¿ç”¨ Hugging Face çš„æ¨¡å‹ä¾†é€²è¡ŒåµŒå…¥
# pip install sentence-transformers  å®‰è£ sentence-transformers å¥—ä»¶ï¼Œç”¨ä¾†é€²è¡Œ Hugging Face æ¨¡å‹çš„åµŒå…¥
# pip install langchain-huggingface  å®‰è£ langchain-huggingface å¥—ä»¶ï¼Œç”¨ä¾†é€²è¡Œ Hugging Face æ¨¡å‹çš„åµŒå…¥
from langchain_huggingface import HuggingFaceEmbeddings

# model_name = "sentence-transformers/all-mpnet-base-v2" # æ•ˆæœä¸ä½³
model_name="intfloat/multilingual-e5-large-instruct" # æ•ˆæœä½³ï¼Œä½†è¦èŠ±æ™‚é–“é™¤éæœ‰ GPU
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': False}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# åˆ©ç”¨ Chroma å»ºç«‹å‘é‡è³‡æ–™åº«ï¼Œä¸¦æŒ‡å®šè³‡æ–™é›†åˆåç¨±èˆ‡åµŒå…¥å‡½å¼
vector_store = Chroma(
    collection_name="test_collection",  # è¨­å®šæ–‡ä»¶é›†åˆåç¨±ç‚º "test_collection"
    embedding_function=hf         # ä½¿ç”¨ä¸Šé¢å»ºç«‹çš„ embeddings ç‰©ä»¶ä½œç‚ºå‘é‡è½‰æ›å‡½å¼
)

# æ¸¬è©¦åˆ—å°å‘é‡è³‡æ–™åº«çš„è³‡æ–™å‹åˆ¥
# print(type(vector_store))  # åˆ—å°å‘é‡è³‡æ–™åº«çš„è³‡æ–™å‹åˆ¥

# å°‡å‰é¢åˆ‡å‰²å¥½çš„æ–‡æª”å€å¡Š texts å­˜å…¥å‘é‡è³‡æ–™åº« vector_store ä¸­ï¼Œä¸¦å°‡å›å‚³çš„æ–‡ä»¶ ID å„²å­˜æ–¼ ids è®Šæ•¸
# è£œå……ï¼šadd_documents(...)æ–¹æ³•æœƒå°‡é€™äº›æ–‡æª”è™•ç†éç¨‹ä¸­ï¼Œé€²è¡Œå‘é‡åµŒå…¥è¨ˆç®—ï¼Œå°‡æ–‡å­—è½‰æˆæ•¸å€¼å‘é‡ï¼Œä¸¦å°‡çµæœå„²å­˜åˆ°å‘é‡è³‡æ–™åº«ä¸­
# è£œå……ï¼šID åœ¨æ—¥å¾Œå¯é€éç›¸ä¼¼åº¦ä¾†æœå°‹æˆ–å…¶ä»–æ“ä½œ
# è£œå……ï¼šç›®å‰æ–‡ä»¶åªå­˜æ”¾åœ¨è¨˜æ†¶é«”ä¸­ï¼Œå°šæœªå¯«å…¥ç¡¬ç¢Ÿ - å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œéœ€å°‡å‘é‡è³‡æ–™åº«å¯«å…¥åˆ°ç¡¬ç¢Ÿ
ids = vector_store.add_documents(texts)

# æ¸¬è©¦åˆ—å°æ–‡æª”å€å¡Šçš„ IDï¼Œä»¥ç¢ºèªæ–‡æª”å·²ç¶“æˆåŠŸå­˜å…¥å‘é‡è³‡æ–™åº«
# richprint(ids)  # åˆ—å°æ–‡æª”å€å¡Šçš„ IDï¼Œä»¥ç¢ºèªæ–‡æª”å·²ç¶“æˆåŠŸå­˜å…¥å‘é‡è³‡æ–™åº«
# print(type(ids))  # åˆ—å°æ–‡æª”å€å¡Šçš„ ID çš„è³‡æ–™å‹åˆ¥
# print(len(ids))  # åˆ—å°æ–‡æª”å€å¡Šçš„ ID çš„æ•¸é‡

# æ¸¬è©¦åœ¨å‘é‡è³‡æ–™åº«ä¸­é€²è¡Œç›¸ä¼¼åº¦æœå°‹ï¼ŒæŸ¥è©¢å•é¡Œç‚ºã€ŒWho invaded Ukraine?ã€
# ä¸¦è¦æ±‚å›å‚³æœ€ç›¸ä¼¼çš„å‰ 2 ç­†çµæœ
'''
results = vector_store.similarity_search(
    'Who invaded Ukraine?',  # æŸ¥è©¢å…§å®¹ï¼šè©¢å•ã€Œèª°å…¥ä¾µäº†çƒå…‹è˜­ï¼Ÿã€
    k=2                      # å›å‚³æœ€ç›¸ä¼¼çš„å‰ 2 ç­†çµæœ
)
# æ¸¬è©¦åˆ—å°ç›¸ä¼¼åº¦æœå°‹çµæœ-å®Œæ•´å…§å®¹
richprint("ç›¸ä¼¼åº¦æœå°‹çµæœï¼š", results)  # åˆ—å°ç›¸ä¼¼åº¦æœå°‹çµæœ

# æ¸¬è©¦åˆ—å°ç›¸ä¼¼åº¦æœå°‹çµæœ-ä¾éœ€æ±‚
for res in results:
    print(f" ğŸ˜„ {res.id} ğŸ˜ [{res.metadata}] ğŸš€ {res.page_content} \n\n")
'''

# ç¬¬4æ­¥-å°‡ Chroma å‘é‡è³‡æ–™åº«è½‰æ›ç‚ºæª¢ç´¢å™¨ï¼Œç”¨æ–¼å¾ŒçºŒçš„è³‡è¨Šæª¢ç´¢è™•ç†
retriever = vector_store.as_retriever()

# æ¸¬è©¦ä½¿ç”¨æª¢ç´¢å™¨é€²è¡ŒæŸ¥è©¢ï¼Œå•é¡Œç‚ºã€ŒWho invaded Ukraine?ã€ï¼Œä¸¦è¦æ±‚å›å‚³æœ€ç›¸ä¼¼çš„å‰ 2 ç­†çµæœ
# query = 'Who invaded Ukraine?'  # æŸ¥è©¢å•é¡Œ
# retriever_results = retriever.invoke(query, k=2)
# richprint("æª¢ç´¢çµæœï¼š", retriever_results)  # åˆ—å°æª¢ç´¢çµæœ

# ç¬¬5æ­¥-å®šç¾©ä¸€å€‹æ–‡ä»¶æ ¼å¼åŒ–å‡½å¼ï¼Œå°‡æª¢ç´¢åˆ°çš„æ–‡æª”æ•´åˆç‚ºä¸€å€‹å­—ä¸²
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)  # æ¯å€‹æ–‡æª”å…§å®¹ä»¥å…©å€‹æ›è¡Œç¬¦è™Ÿé€£æ¥

# ç¬¬6æ­¥-å¼•ç”¨æç¤ºæ¨¡æ¿æ¨¡çµ„ï¼Œå»ºç«‹ç”Ÿæˆæ¨¡å‹éœ€è¦çš„æç¤ºè¨Šæ¯
# pip install langchain-core  å®‰è£ langchain_core å¥—ä»¶ï¼Œç”¨ä¾†å»ºç«‹æç¤ºæ¨¡æ¿
from langchain_core.prompts import PromptTemplate  # åŒ¯å…¥ PromptTemplate é¡åˆ¥

# å»ºç«‹ç”¨ä¾†å¼•å°èªè¨€æ¨¡å‹å›ç­”å•é¡Œçš„æç¤ºæ¨¡æ¿
prompt_template = """
è«‹æ ¹æ“šæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ¶æå‡ºçš„å•é¡Œã€‚å¦‚æœæ‚¨æ ¹æ“šæä¾›çš„ä¸Šä¸‹æ–‡ä¸çŸ¥é“ç­”æ¡ˆï¼Œ
è«‹å‘ŠçŸ¥ç”¨æˆ¶"æ ¹æ“šæä¾›çš„ä¸Šä¸‹æ–‡ç„¡æ³•å›ç­”å•é¡Œ"ï¼Œä¸¦å‘ç”¨æˆ¶è‡´æ­‰ã€‚

ä¸Šä¸‹æ–‡å…§å®¹: {context}

ç”¨æˆ¶çš„å•é¡Œ: {query}

ç­”æ¡ˆ: """
# æ­¤å¤šè¡Œå­—ä¸²æ¨¡æ¿è¦å®šï¼šè‹¥åŸºæ–¼çµ¦å®šçš„ä¸Šä¸‹æ–‡ç„¡æ³•å›ç­”å•é¡Œï¼Œæ¨¡å‹éœ€å›æ‡‰ã€Œä¸çŸ¥é“ã€
# ä¸¦ä»¥æ˜ç¢ºæ ¼å¼å‘ˆç¾ä¸Šä¸‹æ–‡ã€å•é¡Œèˆ‡ç­”æ¡ˆå€å¡Š

# åˆ©ç”¨ä¸Šé¢çš„æ¨¡æ¿å­—ä¸²å»ºç«‹æç¤ºç‰©ä»¶
custom_rag_prompt = PromptTemplate.from_template(prompt_template)

# æ¸¬è©¦åˆ—å°æç¤ºæ¨¡æ¿
# richprint(custom_rag_prompt)  # åˆ—å°æç¤ºæ¨¡æ¿

# ç¬¬7æ­¥-å¼•ç”¨è¼¸å‡ºè§£æå™¨èˆ‡é€šéé‹è¡Œå™¨æ¨¡çµ„ï¼Œå»ºç«‹ RAG éˆ-æ•´åˆæª¢ç´¢ã€æç¤ºæ¨¡æ¿ã€ç”ŸæˆåŠè§£æç­‰æµç¨‹
# StrOutputParser ç”¨æ–¼è§£æèªè¨€æ¨¡å‹å›å‚³çš„ç´”æ–‡å­—ç­”æ¡ˆï¼Œ
# RunnablePassthrough ç”¨æ–¼ç›´æ¥å‚³éæŸ¥è©¢æ–‡å­—ï¼Œä¸åšä»»ä½•è½‰æ›
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# å»ºç«‹ RAGï¼ˆæª¢ç´¢èˆ‡ç”Ÿæˆï¼‰éˆï¼Œæ•´åˆæª¢ç´¢ã€æç¤ºæ¨¡æ¿ã€ç”ŸæˆåŠè§£æç­‰æµç¨‹
rag_chain = (
    {"context": retriever | format_docs, "query": RunnablePassthrough()}  # å°‡ã€Œcontextã€æ¬„ä½è¨­ç‚ºç”±æª¢ç´¢å™¨å–å¾—å¾Œå†æ ¼å¼åŒ–æ–‡ä»¶ï¼Œ
    | custom_rag_prompt                                               # å°‡æŸ¥è©¢èˆ‡ä¸Šä¸‹æ–‡è³‡è¨Šå¥—ç”¨åˆ°è‡ªå®šæç¤ºæ¨¡æ¿ä¸Š
    | groq_generate_response                                          # å‘¼å« openai_generate_response ç”Ÿæˆæ¨¡å‹å›ç­”
    | StrOutputParser()                                                 # è§£æç”Ÿæˆæ¨¡å‹å›æ‡‰ç‚ºç´”æ–‡å­—ç­”æ¡ˆ
)

# æ¸¬è©¦ä½¿ç”¨ RAG éˆé€²è¡ŒæŸ¥è©¢ï¼Œå•é¡Œå…§å®¹æ ¹æ“š 2024 åœ‹æƒ…å’¨æ–‡ï¼Œä¸”è¦æ±‚æ¨¡å‹ä»¥ä¸­æ–‡å›ç­”
# åˆ©ç”¨ RAG éˆé€²è¡ŒæŸ¥è©¢ï¼Œå•é¡Œå…§å®¹æ ¹æ“š 2024 ç¾åœ‹åœ‹æƒ…å’¨æ–‡ï¼Œä¸”è¦æ±‚æ¨¡å‹ä»¥ä¸­æ–‡å›ç­”
ans1 = rag_chain.invoke("æ ¹æ“š2024å¹´çš„åœ‹æƒ…å’¨æ–‡æ¼”è¬›ï¼Œèª°å…¥ä¾µäº†çƒå…‹è˜­ï¼Ÿ è«‹ç”¨ç¹é«”ä¸­æ–‡å°ç£ç”¨èªå›ç­”")
print(ans1)  # å°‡ç¬¬ä¸€å€‹æŸ¥è©¢çš„çµæœå°å‡º

# é€²è¡Œç¬¬äºŒå€‹æŸ¥è©¢ï¼Œå•é¡Œç‚ºã€Œç”Ÿå‘½çš„æ„ç¾©ç‚ºä½•ï¼Ÿã€ç¨‹å¼é æœŸæ¨¡å‹æœƒå›ç­”ã€Œä¸çŸ¥é“ã€
ans2 = rag_chain.invoke("ç”Ÿå‘½çš„æ„ç¾©æ˜¯ä»€éº¼ï¼Ÿ è«‹ç”¨ç¹é«”ä¸­æ–‡å°ç£ç”¨èªå›ç­”")
print(ans2)  # å°‡ç¬¬äºŒå€‹æŸ¥è©¢çš„çµæœå°å‡º
